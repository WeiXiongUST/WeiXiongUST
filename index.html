<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Wei Xiong</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="Bio.html">Biography</a></div>
<div class="menu-item"><a href="blog.html">Blog</a></div>
<div class="menu-item"><a href="research.html">Publication</a></div>
<div class="menu-category">Miscellaneous</div>
<div class="menu-item"><a href="Miscellaneous.html">Misc.</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Wei Xiong</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photo_gg.jpeg" alt="picture" width="250px" height="250px" />&nbsp;</td>
<td align="left"><p>Ph.D. student<br /></p>
<a href="https://cs.illinois.edu/" target=&ldquo;blank&rdquo;>University of Illinois Urbana-Champaign, Computer Science</a><br /></p>
<p>Email: wx13 AT illinois DOT edu </a><br /></p>
<p><a href="cv.pdf" onclick="javascript:urchinTracker('/downloads/cv.pdf');">[Curriculum Vitae]</a> &nbsp;&nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=m2-OwQEAAAAJ&amp;hl=zh-CN" target=&ldquo;blank&rdquo;>[Google Scholar]</a></p>
</td></tr></table>
<p>I am currently a first-year Ph.D. student in computer science at UIUC, working with <a href="http://tongzhang-ml.org/" target=&ldquo;blank&rdquo;>Prof. Tong Zhang</a> and <a href="http://nanjiang.cs.illinois.edu" target=&ldquo;blank&rdquo;>Prof. Nan Jiang</a>. I also concurrently work with <a href="https://deepmind.google" target=&ldquo;blank&rdquo;>Google Deepmind</a> as a full-time/20% part-time student researcher since 2024.5. </a>q</p>
<p>Prior to this, I received a master's degree in mathematics in 2023 from The Hong Kong University of Science and Technology, where my study was supported by the Hong Kong PhD Fellowship. I enjoyed several fabulous years at the University of Science and Technology of China and obtained a B.S. in mathematics in 2021, where I worked closely with <a href="http://www.ece.virginia.edu/~cs7dt/home.html" target=&ldquo;blank&rdquo;>Prof. Cong Shen</a>. I worked as a research intern at the deep reinforcement learning team in <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" target=&ldquo;blank&rdquo;>Microsoft Research Asia</a>.</p>  
<h2>Research Interests</h2>
<p>My research lies at the intersection of machine learning and decision making problems. The long-term goal of my research is to build strong agents that can make adaptive decisions tailored to dynamic contexts and specified criteria.</p> 
<p>The recent breakthroughs in foundation large language models (LLMs) have further sparked my passion to develop LLM-based agent. I seek to understand the mathematical principle behind the problems, develop scalable and efficient algorithms from the underlying theoretical insights, and make these techniques available to the community. I list some representative research problems and projects here.
<ol>
<li><p><b>Algorithm designs in reinforcement learning from human feedback (RLHF)</b> <br /> 
We propose several complementary or alternative algorithms of Proximal Policy Optimization (PPO) for the post-training of large language models, including [<a href="https://arxiv.org/pdf/2304.06767" target=&ldquo;blank&rdquo;>Iterative best-of-n fine-tuning</a>], [<a href="https://arxiv.org/pdf/2312.11456" target=&ldquo;blank&rdquo;>Iterative direct preference learning</a>], [<a href="https://arxiv.org/abs/2309.06256" target=&ldquo;blank&rdquo;>Adaptive Model Averaging</a>]. In particular, our studies are the first line of works to illustrate the power of iterative and on-policy direct preference learning, as compared to the offline counterparts. These algorithms are crucial components of the modern alignment frameworks adopted by projects such as LLaMA or Qwen. <br /> <br /></p>
</li>
<li><p><b>Mathematical principle of decision making porblems</b><br /> 
<ol>
<li>We make the first attempt to formally formulate the RLHF as a <a href="https://arxiv.org/pdf/2312.11456" target=&ldquo;blank&rdquo;>KL-regularized contextual bandit</a> (for Bradley-Terry model) and a <a href="https://arxiv.org/pdf/2402.07314" target=&ldquo;blank&rdquo;>KL-regularized two-player minimax game</a> (for general preference model). We investigate its learnability in three distinct settings—offline, online, and hybrid—and propose efficient algorithms with finite-sample theoretical guarantees. <br /><br /></p></li>
<li> In [<a href="https://arxiv.org/pdf/2211.01962" target=&ldquo;blank&rdquo;>ZXZWWYZ22</a>] and [<a href="https://weixiongust.github.io/WeiXiongUST/masterthesis.pdf" target=&ldquo;blank&rdquo;>X23</a>], we show that the interactive decision-making problems can be reduced to a supervised online estimation, providing a unified understanding of Markov decision process (MDP), partially observable MDP, and two-player zero-sum Markov game. In particular, the widely adopted optimism principle serves to empower such a reduction and the complexity of the problem can be characterized by the cost of such a reduction. The framework covers nearly all known solvable interactive decision-making problems. This framework also leads to the development of a new optimization-based approach that achieves optimism through biased loss functions, offering an alternative to traditional constrained optimistic planning.<br /><br /></p></li>
<li> In [<a href="https://arxiv.org/pdf/2205.15512" target=&ldquo;blank&rdquo;>XZSSWZ22</a>] and [<a href="https://arxiv.org/pdf/2202.07511" target=&ldquo;blank&rdquo;>ZXTWZWY22</a>], we conduct initial studies on what dataset permits solving offline multi-agent RL problems. While the single-agent setting benefits from covering the single distribution induced by the optimal policy, our research reveals that covering the Nash equilibrium is not sufficient and a much stronger condition is necessary for multi-agent scenarios, even for the simplest two-player zero-sum Markov game.</li>
</ol>
<li><p><b>Preferece signal modeling</b><br /> 
<ol>
<li> We open-source a series of <a href="https://huggingface.co/sfairXC/FsfairX-LLaMA3-RM-v0.1" target=&ldquo;blank&rdquo;>Bradley-Terry reward model</a> and <a href="https://huggingface.co/RLHFlow/pair-preference-model-LLaMA3-8B" target=&ldquo;blank&rdquo;>pair-wise preference model</a>, which achieve state-of-the-art performance of their class, as evaluated by the <a href="https://huggingface.co/spaces/allenai/reward-bench" target=&ldquo;blank&rdquo;>Reward bench</a>. <br /><br /></p></li>
<li> We study the <a href="https://huggingface.co/RLHFlow/ArmoRM-Llama3-8B-v0.1" target=&ldquo;blank&rdquo;>multi-head reward modeling</a> from different criterion like helpfulness, safety, and verbosity, as well as the algorithm design under this framework. We also develop a mixture-of-expert approach to aggregate the reward signals where a gating network decides the combination coefficient by the embedding of the prompt. Our resulting model, significantly outperforms competing models in its class with a more fine-grained reward signal, and approaches the performance of the much larger Nemotron-4 340B.<br /><br /></p></li>
<li> The resulting models have received significant attention across industry and academic circles, with either public follow-up works or private communications with major entities such as Google, Microsoft Research, Amazon, Meta FAIR, as well as universities like KAIST, Northwestern, UIUC, and Princeton.</li>
</ol>
  </li>
<li><p><b>Bringing the techniques to the community</b><br /> 
<ol>
<li> I initialize and organize the <a href="https://github.com/RLHFlow" target=&ldquo;blank&rdquo;>RLHFlow project</a>, which received ~1K GitHub stars so far. We present a step by step guidance to do the post training of LLMs: suervised fine-tuning, reward modeling, and online iterative direct preference learning. Our training recipe is based only on the open-source dataset and the resulting model achieves state-of-the-art performance in its class. <br /><br /></p></li>
<li> I am also a founding member and core contributor of the <a href="https://github.com/OptimalScale/LMFlow" target=&ldquo;blank&rdquo;>LMFlow project</a>, which received 8.2K GitHub stars so far. We present an extensible toolkit for finetuning and inference of LLMs. I am responsible for the RLHF part of the project. <br /><br /></p></li>
</ol>
  </li>
</ol>

<p>If you are interested in discussing with me, feel free to drop me an email or add my wechat wei_xiong2000</p>
<div id="footer">
<div id="footer-text">
Page generated 2023-08-04, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>

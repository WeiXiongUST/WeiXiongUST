<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Wei Xiong</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="Bio.html">Biography</a></div>
<div class="menu-item"><a href="blog.html">Blog</a></div>
<div class="menu-item"><a href="research.html">Publication</a></div>
<div class="menu-category">Miscellaneous</div>
<div class="menu-item"><a href="Miscellaneous.html">Misc.</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Wei Xiong</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photo_gg.jpg" alt="picture" width="250px" height="250px" />&nbsp;</td>
<td align="left"><p>Ph.D. student<br /></p>
<a href="https://cs.illinois.edu/" target=&ldquo;blank&rdquo;>University of Illinois Urbana-Champaign, Computer Science</a><br /></p>
<p><a href="cv.pdf" onclick="javascript:urchinTracker('/downloads/cv.pdf');">[Curriculum Vitae]</a> &nbsp;&nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=m2-OwQEAAAAJ&amp;hl=zh-CN" target=&ldquo;blank&rdquo;>[Google Scholar]</a></p>
</td></tr></table>
<p>I am currently a first-year Ph.D. student in computer science at UIUC, working with <a href="http://tongzhang-ml.org/" target=&ldquo;blank&rdquo;>Prof. Tong Zhang</a> and <a href="http://nanjiang.cs.illinois.edu" target=&ldquo;blank&rdquo;>Prof. Nan Jiang</a>. I also work for Google Deepmind as a student researcher since 2024.5</a>.</p>
<p>Prior to this, I received a master's degree in mathematics in 2023 from The Hong Kong University of Science and Technology. I enjoyed several fabulous years at the University of Science and Technology of China and obtained a B.S. in mathematics in 2021, where I worked closely with <a href="http://www.ece.virginia.edu/~cs7dt/home.html" target=&ldquo;blank&rdquo;>Prof. Cong Shen</a>.</p>  
<h2>Research Interests</h2>
<p>I study the post-training of LLMs, with a focus on reinforcement learning from human feedback (RLHF). Previously, I have spent time on the mathematical foundation of RL, where I was fortunate to collaborate with many great senior mentors and talented peers. I also spent time on deep RL at Microsoft Research Asia. </p>
<p>My primary research goal is to develop practical and theoretically sound algorithms and make these techniques accessible to the community.</p>  
<div class="infoblock">
<div class="blocktitle">News</div>
<div class="blockcontent">
<p>We formulate the real-world RLHF process as a reverse-KL regularized contextual bandits and study its theoretical property by proposing statistically efficient algorithms with finite-sample theoretical guarantee. We also connect our theoretical findings with practical algorithms (e.g. DPO, RSO), offering new tools and insights for the algorithmic design of alignment algorithms. Check out our <a href="https://arxiv.org/abs/2312.11456" target=&ldquo;blank&rdquo;>technical report</a>. </a>.</p>
</div></div>

<h2>Selected Projects</h2>
<p>(α-β) means that the order is decided by rock-paper-scissors and (*) denotes equal contribution.</p>
<ol>
<li><p><a href="https://arxiv.org/abs/2405.07863" target=&ldquo;blank&rdquo;>RLHF Workflow: From Reward Modeling to Online RLHF</a> [<a href="https://github.com/RLHFlow/Online-RLHF" target=&ldquo;blank&rdquo;>Code</a>] <br /> 
(α-β) Hanze Dong*, Wei Xiong*, Bo Pang*, Haoxiang Wang*, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, Tong Zhang, Technical Report <br /> 
An open-source recipe to do online RLHF and make the state-of-the-art reward models and chatbot. <br /><br /></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2406.12845" target=&ldquo;blank&rdquo;>Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts </a> [<a href="https://huggingface.co/RLHFlow/ArmoRM-Llama3-8B-v0.1" target=&ldquo;blank&rdquo;>Model</a>] <br /> 
Haoxiang Wang*, Wei Xiong*, Tengyang Xie, Han Zhao, Tong Zhang, Technical Report <br /> 
A multi-head reward model with mixture-of-expert-style aggregation strategy. Best open-source RM of its class! <br /><br /></p>
</li>
<li><p><a href="gshf.pdf" onclick="javascript:urchinTracker('/downloads/cv.pdf');">Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint</a> <a href="sliderlhf.pdf" onclick="javascript:urchinTracker('/downloads/sliderlhf.pdf');">[Slide] </a>  [<a href="https://github.com/RLHFlow/Online-RLHF" target=&ldquo;blank&rdquo;>Code</a>] <br />
Wei Xiong*, Hanze Dong*, Chenlu Ye*, Han Zhong, Nan Jiang, Tong Zhang, ICML 2024, also Oral Presentation at ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models. <br /> 
Online iterative DPO; Mathematical foundation of RLHF as a KL-regularized contextual bandit.
<br /><br /></p>
  </li>
<li><p><a href="https://arxiv.org/pdf/2402.18571v1.pdf" target=&ldquo;blank&rdquo;>Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards</a> <a href="sliderlhf.pdf" onclick="javascript:urchinTracker('/downloads/sliderlhf.pdf');">[Slide] </a> [<a href="https://github.com/RLHFlow/Directional-Preference-Alignment" target=&ldquo;blank&rdquo;>Code</a>] <br />
Haoxiang Wang*, Yong Lin*, Wei Xiong*, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, Tong Zhang, ACL 2024 <br /> 
A user-preference-aware alignmnet framework under multi-objective reward formulation.
<br /><br /></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2402.07314.pdf" target=&ldquo;blank&rdquo;>Online Iterative Reinforcement Learning from Human Feedback with General Preference Model </a> <a href="sliderlhf.pdf" onclick="javascript:urchinTracker('/downloads/sliderlhf.pdf');">[Slide] </a> <br />
(α-β) Chenlu Ye*, Wei Xiong*, Yuheng Zhang*, Nan Jiang, Tong Zhang, Preprint. <br /> 
Learnability of RLHF under general reward-model-free preference structure.
<br /><br /></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2309.06256.pdf" target=&ldquo;blank&rdquo;>Mitigating the Alignment Tax of RLHF</a> <br />
(α-β) Yong Lin*, Hangyu Lin*, Wei Xiong*, Shizhe Diao*, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, Hanze Dong, Renjie Pi, Han Zhao, Nan Jiang, Yuan Yao, Heng Ji, and Tong Zhang, Preprint. <br /> 
Model merge for mitigating alignment tax.
<br /><br /></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2304.06767" target=&ldquo;blank&rdquo;>RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment</a> [<a href="https://github.com/RLHFlow/Online-RLHF" target=&ldquo;blank&rdquo;>Code</a>] <br />
(α-β) Hanze Dong*, Wei Xiong*,  Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum and Tong Zhang, TMLR. <br /> 
We propose RAFT to iteratively learn from best-of-n sampling. It is also known as rejection sampling fine-tuning.
<br /><br /></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.12420" target=&ldquo;blank&rdquo;>LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models</a> [<a href="https://github.com/OptimalScale/LMFlow" target=&ldquo;blank&rdquo;>Code</a>] <br />
Shizhe Diao, Rui Pan, Hanze Dong, Ka Shun Shum, Jipeng Zhang, Wei Xiong, Tong Zhang, NAACL 2024 <br />
A framework to develop LLMs (fine-tuning, inference, RLHF&hellip;) with 8K+ star in github. I was responsible for developing the RLHF part of the project.
<br /> <br /></p>
</li>
<li><p><a href="masterthesis.pdf" onclick="javascript:urchinTracker('/downloads/slidegfarl.pdf');">Master thesis: A Sufficient Condition of Sample-Efficient Reinforcement Learning with General Function Approximation </a><br />
An introduction to eluder coefficient, and also a new confidence-set-free algorithmic framework based on optimism (long version accepted to NeurIPS).
<br /> <br /></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.01962" target=&ldquo;blank&rdquo;>GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond</a> <a href="slideonlinegfa.pdf" onclick="javascript:urchinTracker('/downloads/slidegfarl.pdf');">[Slide] </a><br />
(α-β) Han Zhong*, Wei Xiong*, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang and Tong Zhang, Preprint.<br />
A unified viewpoint of solvable decision-making problems: limited generalization in an online manner, and a reduction from interactive decision making to the offline supervised learning. Also a unified algorithmic framework for both posterior sampling and UCB algorithm. <br /> <br /></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.05949" target=&ldquo;blank&rdquo;>Corruption-Robust Algorithms with Uncertainty Weighting for Nonlinear Contextual Bandits and Markov Decision Processes</a> <br />
Chenlu Ye, Wei Xiong, Quanquan Gu and Tong Zhang, <i>ICML</i> 2023.<br /> 
Weighted regression in the face of adversarial corruptions: new weight design, and new techniques for controlling the sum of weight (counterpart of the elliptical potential lemmas).  
<br /> <br /></p>
</li>
<li><p><a href="offlinelinearrl.pdf" onclick="javascript:urchinTracker('/downloads/cv.pdf');">Nearly Minimax Optimal Offline Reinforcement Learning with Linear Function Approximation: MDP and MG</a> <a href="slideoffline.pdf" onclick="javascript:urchinTracker('/downloads/slidegfarl.pdf');">[Slide] </a><br />
Wei Xiong*, Han Zhong*, Chengshuai Shi, Cong Shen, Liwei Wang, and Tong Zhang,  <i>ICLR</i> 2023. <br />
An application of weighted regression by using the variance information to achieve a sharper bound. <br /><br /></p>
</li>
<li><p><a href="https://arxiv.org/abs/2012.15010" target=&ldquo;blank&rdquo;>PMGT-VR: A decentralized proximal-gradient algorithmic framework with variance reduction</a>  <a href="slideopt.pdf" onclick="javascript:urchinTracker('/downloads/slide<u>gfa</u>rl.pdf');">[Slide] </a><br /> 
Haishan Ye*, Wei Xiong*, and Tong Zhang,  Under Minor Revision at <i>TPAMI</i>. <br />
A unified algorithmic and analysis framework for decentralized variance-reduction algorithm with matching convergence rate of their centralized counterparts.
<br /> <br /></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.14622" target=&ldquo;blank&rdquo;>Heterogeneous Multi-player Multi-armed Bandits: Closing the Gap and Generalization</a>  [<a href="https://github.com/ShenGroup/MPMAB_BEACON" target=&ldquo;blank&rdquo;>Code</a>] <br /> 
Chengshuai Shi, Wei Xiong, Cong Shen, and Jing Yang, <i>NeurIPS</i>, 2021.<br />
A new algorithm BEACON with batch exploration and a carefully crafted communication strategy: minimax-optimal regret bound and also impressive empirical performance.
<br /> <br /></p>
</li>
</ol>
<h2>Misc</h2>
<p>I take notes while learning &hellip;</p>
<p><a href="note_exp.pdf" onclick="javascript:urchinTracker('/downloads/cv.pdf');">1 Note on exponential inequality</a></p>
<p><a href="note_entropy.pdf" onclick="javascript:urchinTracker('/downloads/cv.pdf');">2 Note on metric entropy</a></p>
<p><a href="note_stability.pdf" onclick="javascript:urchinTracker('/downloads/cv.pdf');">3 An alternative analysis of high-probability generalization bound for uniformly stable algorithms</a></p>
<p><a href="note_minimax.pdf" onclick="javascript:urchinTracker('/downloads/cv.pdf');">4 Note on minimax lower bound</a></p>
<p><a href="note_reduction.pdf" onclick="javascript:urchinTracker('/downloads/cv.pdf');">5 Note on reduction-based RL</a></p>
<p><a href="notecontextualbandit.pdf" onclick="javascript:urchinTracker('/downloads/cv.pdf');">6 Note on non-linear contextual bandit</a></p>
<p><a href="notemartingale.pdf" onclick="javascript:urchinTracker('/downloads/cv.pdf');">7 Note on martingale concentration inequality</a></p>
<p>If you are interested in discussing with me, feel free to drop me an email or add my wechat wei_xiong2000</p>
<h2>Contact</h2>
<p>Email: wx13 AT illinois DOT edu </p>
<div id="footer">
<div id="footer-text">
Page generated 2023-08-04, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>

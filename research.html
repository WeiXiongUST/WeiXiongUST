<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=GBK" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Research</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="Bio.html">Biography</a></div>
<div class="menu-item"><a href="blog.html">Blog</a></div>
<div class="menu-item"><a href="research.html">Publication</a></div>
<div class="menu-category">Miscellaneous</div>
<div class="menu-item"><a href="Miscellaneous.html">Misc.</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research</h1>
</div>
<h2>Papers and Preprints</h2>
<ol>
<li><p><a href="https://arxiv.org/abs/2405.07863" target=&ldquo;blank&rdquo;>RLHF Workflow: From Reward Modeling to Online RLHF</a> [<a href="https://github.com/RLHFlow/Online-RLHF" target=&ldquo;blank&rdquo;>Code</a>] <br /> 
(α-β) Hanze Dong*, Wei Xiong*, Bo Pang*, Haoxiang Wang*, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, Tong Zhang, Technical Report <br /> 
An open-source recipe to do online RLHF and make the state-of-the-art reward models and chatbot. <br /><br /></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2406.12845" target=&ldquo;blank&rdquo;>Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts </a> [<a href="https://huggingface.co/RLHFlow/ArmoRM-Llama3-8B-v0.1" target=&ldquo;blank&rdquo;>Model</a>] <br /> 
Haoxiang Wang*, Wei Xiong*, Tengyang Xie, Han Zhao, Tong Zhang, Technical Report <br /> 
A multi-head reward model with mixture-of-expert-style aggregation strategy. Best open-source RM of its class! <br /><br /></p>
</li>
<li><p><a href="gshf.pdf" onclick="javascript:urchinTracker('/downloads/cv.pdf');">Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint</a> <a href="sliderlhf.pdf" onclick="javascript:urchinTracker('/downloads/sliderlhf.pdf');">[Slide] </a>  [<a href="https://github.com/RLHFlow/Online-RLHF" target=&ldquo;blank&rdquo;>Code</a>] <br />
Wei Xiong*, Hanze Dong*, Chenlu Ye*, Han Zhong, Nan Jiang, Tong Zhang, ICML 2024, also Oral Presentation at ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models. <br /> 
Online iterative DPO; Mathematical foundation of RLHF as a KL-regularized contextual bandit.
<br /><br /></p>
  </li>
<li><p><a href="https://arxiv.org/pdf/2402.18571v1.pdf" target=&ldquo;blank&rdquo;>Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards</a> <a href="sliderlhf.pdf" onclick="javascript:urchinTracker('/downloads/sliderlhf.pdf');">[Slide] </a> [<a href="https://github.com/RLHFlow/Directional-Preference-Alignment" target=&ldquo;blank&rdquo;>Code</a>] <br />
Haoxiang Wang*, Yong Lin*, Wei Xiong*, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, Tong Zhang, ACL 2024 <br /> 
A user-preference-aware alignmnet framework under multi-objective reward formulation.
<br /><br /></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2402.07314.pdf" target=&ldquo;blank&rdquo;>Online Iterative Reinforcement Learning from Human Feedback with General Preference Model </a> <a href="sliderlhf.pdf" onclick="javascript:urchinTracker('/downloads/sliderlhf.pdf');">[Slide] </a> <br />
(α-β) Chenlu Ye*, Wei Xiong*, Yuheng Zhang*, Nan Jiang, Tong Zhang, Preprint. <br /> 
Learnability of RLHF under general reward-model-free preference structure.
<br /><br /></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2309.06256.pdf" target=&ldquo;blank&rdquo;>Mitigating the Alignment Tax of RLHF</a> <br />
(α-β) Yong Lin*, Hangyu Lin*, Wei Xiong*, Shizhe Diao*, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, Hanze Dong, Renjie Pi, Han Zhao, Nan Jiang, Yuan Yao, Heng Ji, and Tong Zhang, Preprint. <br /> 
Model merge for mitigating alignment tax.
<br /><br /></p>
<li><p><a href="https://arxiv.org/pdf/2304.06767" target=&ldquo;blank&rdquo;>RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment</a> <br />
Hanze Dong*, Wei Xiong*,  Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum and Tong Zhang, TMLR. <br /> <br /></p>
We propose RAFT to iteratively learn from best-of-n sampling. It is also known as rejection sampling fine-tuning. <br /> <br /></p>
</li>
<li><p><a href="masterthesis.pdf" onclick="javascript:urchinTracker('/downloads/slidegfarl.pdf');">A Sufficient Condition of Sample-Efficient Reinforcement Learning with General Function Approximation </a><br />
Wei Xiong, Master thesis<br /><br /></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2305.18258.pdf" target=&ldquo;blank&rdquo;>One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration</a> <br />
Zhihan Liu*, Miao Lu*, Wei Xiong*, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, Zhaoran Wang, NeurIPS 2023 <br /> <br /></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.01962" target=&ldquo;blank&rdquo;>GEC: A Unified Framework for Interactive Decision Making in MDP, POMDP, and Beyond</a> <a href="slideonlinegfa.pdf" onclick="javascript:urchinTracker('/downloads/slidegfarl.pdf');">[Slide] </a><br />
Han Zhong*, Wei Xiong*, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang and Tong Zhang, Preprint.<br /> <br /></p>
</li>
<li><p><a href="https://arxiv.org" target=&ldquo;blank&rdquo;>Reward Teaching for Federated Multi-Armed Bandits</a><br />
Chengshuai Shi, Wei Xiong, Cong Shen, and Jing Yang, IEEE Transaction on Signal Processing. <br /></p>
A short version accepted by IEEE International Symposium on Information Theory (ISIT 2023). <br /><br /></p>
</li>
<li><p><a href="https://arxiv.org" target=&ldquo;blank&rdquo;>Provably Efficient Offline Reinforcement Learning with Perturbed Data Sources</a><br />
Chengshuai Shi, Wei Xiong, Cong Shen, and Jing Yang, <i>ICML</i> 2023.<br /><br /></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.05949" target=&ldquo;blank&rdquo;>Corruption-Robust Algorithms with Uncertainty Weighting for Nonlinear Contextual Bandits and Markov Decision Processes</a> <br />
Chenlu Ye, Wei Xiong, Quanquan Gu and Tong Zhang, <i>ICML</i> 2023.<br /> <br /></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.15512" target=&ldquo;blank&rdquo;>Nearly Minimax Optimal Offline Reinforcement Learning with Linear Function Approximation: Single-Agent MDP and Markov Game</a> <a href="slideoffline.pdf" onclick="javascript:urchinTracker('/downloads/slidegfarl.pdf');">[Slide] </a><br />
Wei Xiong*, Han Zhong*, Chengshuai Shi, Cong Shen, Liwei Wang, and Tong Zhang,  <i>ICLR</i> 2023. <br /><br /></p>
</li>
<li><p><a href="https://proceedings.mlr.press/v162/xiong22b/xiong22b.pdf" target=&ldquo;blank&rdquo;>A Self-Play Posterior Sampling Algorithm for Zero-Sum Markov Game</a> <br />
Wei Xiong, Han Zhong, Chengshuai Shi, Cong Shen, and Tong Zhang, <i>ICML</i> 2022.<br /> <br /></p>
</li>
<li><p><a href="https://arxiv.org/abs/2202.07511" target=&ldquo;blank&rdquo;>Pessimistic Minimax Value Iteration: Provably Efficient Equilibrium Learning from Offline Datasets</a> <br />
Han Zhong*, Wei Xiong*, Jiyuan Tan*, Liwei Wang, Tong Zhang, Zhaoran Wang, and Zhuoran Yang, <i>ICML</i> 2022. <br /><br /></p>
</li>
<li><p><a href="note_stability.pdf" onclick="javascript:urchinTracker('/downloads/cv.pdf');">An Alternative Analysis of High-Probability Generalization Bound for Uniformly Stable Algorithms</a><br />
Wei Xiong, Yong Lin, and Tong Zhang, Project Report, Not intended for publication.<br /><br /></p>
</li>
<li><p><a href="https://arxiv.org/abs/2012.15010" target=&ldquo;blank&rdquo;>PMGT-VR: A decentralized proximal-gradient algorithmic framework with variance reduction</a>  <a href="slideopt.pdf" onclick="javascript:urchinTracker('/downloads/slide<u>gfa</u>rl.pdf');">[Slide] </a><br /> 
Haishan Ye*, Wei Xiong*, and Tong Zhang,  Under Minor Revision at <i>TPAMI</i>. <br /> <br /></p>
</li>
<li><p><a href="undergraduatethesis.pdf" onclick="javascript:urchinTracker('/downloads/cv.pdf');">Algorithms of Multi-player Multi-armed Bandits under Different Settings</a><br />
Wei Xiong, <i>Undergraduate Thesis</i> (in Chinese).<br /> <br /></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.14622" target=&ldquo;blank&rdquo;>Heterogeneous Multi-player Multi-armed Bandits: Closing the Gap and Generalization</a>  [<a href="https://github.com/ShenGroup/MPMAB_BEACON" target=&ldquo;blank&rdquo;>Code</a>] <br /> 
Chengshuai Shi, Wei Xiong, Cong Shen, and Jing Yang, <i>NeurIPS</i>, 2021.<br /> <br /></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.14628" target=&ldquo;blank&rdquo;>(Almost) Free Incentivized Exploration from Decentralized Learning Agents</a>  [<a href="https://github.com/ShenGroup/Observe_then_Incentivize" target=&ldquo;blank&rdquo;>Code</a>] <br /> 
Chengshuai Shi, Haifeng Xu, Wei Xiong, and Cong Shen, <i>NeurIPS</i>, 2021.<br /> <br /></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.13578" target=&ldquo;blank&rdquo;>Distributional Reinforcement Learning for Multi-Dimensional Reward Functions</a>  <br /> 
Pushi  Zhang,  Xiaoyu  Chen,  Li  Zhao,  Wei  Xiong,  Tao  Qin,  and  Tie-Yan  Liu, <i>NeurIPS</i>, 2021.<br /> <br /></p>
</li>
<li><p><a href="https://arxiv.org/abs/2003.00162" target=&ldquo;blank&rdquo;>Decentralized multi-player multi-armed bandits with no collision information</a>  [<a href="https://github.com/WeiXiongUST/multi_player_multi_armed_bandit_algorithms" target=&ldquo;blank&rdquo;>Code*</a>] <br /> 
Chengshuai Shi, Wei Xiong, Cong Shen, and Jing Yang, <i>AISTATS</i>, 2020.<br /> <br /></p>
</li>
</ol>
<p>(* equal contribution or alphabetical order)</p>
<p>The Code* implementes many SOTA and baseline MPMAB algorithms, which is a nice work of Cindy Trinh.</p>
</td>
</tr>
</table>
</body>
</html>
